{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5127ba-e79e-4a1d-8e31-4a30cb754d91",
   "metadata": {},
   "source": [
    "# Aqui van las notas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149faaa-b8c6-475d-adf4-7426e45a457e",
   "metadata": {},
   "source": [
    "## Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384cb297-e634-4d8c-bc96-ab41baa44edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3284e6-f6fc-4dd9-b27a-9cc5ca5518ac",
   "metadata": {},
   "source": [
    "Creando un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ed7cf4-9b1c-448e-93af-6ed0d6ae65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((8, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf18c9f-ad0d-4ab9-9efc-abd36db452e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "505dc00f-c4ee-49d3-ab86-b639c8f223cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.size() == (8, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672bac8-42ad-48b5-b72c-68bfa34f6658",
   "metadata": {},
   "source": [
    "Acceder a los elementos del tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911bfd33-92e8-4770-9b16-66ce928b2937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8644)\n",
      "tensor(0.8644)\n",
      "tensor([0.8644, 0.5202, 0.8190])\n",
      "tensor([0.8644, 0.9258, 0.5758, 0.3023, 0.8031, 0.4529, 0.9669, 0.1442])\n"
     ]
    }
   ],
   "source": [
    "print(A[0][0][0]) # De la manera mas fea\n",
    "\n",
    "print(A[0, 0, 0]) # Mas simple\n",
    "\n",
    "print(A[0, :, 0]) # Slicing\n",
    "\n",
    "print(A[:, 0, 0]) # Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a15ef9f-0767-4d41-ad0c-4c41b84846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52283f4-124f-4433-99d4-cd840f099baf",
   "metadata": {},
   "source": [
    "Vamos a ver su tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7740b9-c4f3-40d4-b6da-0751dfff3769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed9fa1c6-1389-4119-9fad-d44f33494af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 1],\n",
       "        [0, 1, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = torch.randint(2, (5, 5))\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60f64b11-80e6-4f5e-9c46-6fbaf44f1eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3226da55-c801-4476-8c04-5bc67e71c950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.],\n",
       "        [0., 1., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 1.],\n",
       "        [0., 1., 1., 0., 1.],\n",
       "        [1., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C + D # Suma de tensores sin importar tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a63d157b-684b-4c51-b18e-236561647a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.ones((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b16c7ed-4b3f-4b9c-8850-d0c5b79d4643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "072d2d5f-f158-4d39-8aec-50800f86c9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 2., 1., 1.],\n",
       "        [1., 2., 2., 1., 1.],\n",
       "        [1., 2., 1., 1., 2.],\n",
       "        [1., 2., 2., 1., 2.],\n",
       "        [2., 2., 1., 2., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C + D + E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669b606-d0b5-4208-88e5-1fad9815f3e9",
   "metadata": {},
   "source": [
    "Como pasar tensores en GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50c365c-20fe-4c6d-a6bf-1505d55e3dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.device # checamos en donde esta el tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fba3a-9730-4c84-8914-7472d6c3b6a0",
   "metadata": {},
   "source": [
    "Para revisar si tenemos la GPU disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccee2bb6-c095-4cd7-b819-3cc37cadde25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65e0fd-60af-4cb8-87fe-b0e35fe3f968",
   "metadata": {},
   "source": [
    "Para definir el dispositivo a utilizar, vamos a hacer lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67001fe8-4cc0-4fa1-9e44-4bc45528a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a trabajar con la GPU siempre que este disponible\n",
    "# de lo contrario usamos la CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86856fa1-c88f-4bc5-aa5a-663bcd9ecf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b593177f-b67d-49d0-9f1d-94233bda2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 5) # cpu\n",
    "# gpu\n",
    "y = torch.ones_like(x, device=device) # Toma la dimension del argumento y lo rellena de 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92884bb5-3151-42bf-b5ae-11d83716d8eb",
   "metadata": {},
   "source": [
    "Para operar $x$ y $y$ tenemos que tener los tensores en el mismo device,\n",
    "por lo que vamos a pasar a $x$ a la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "364ef973-4d9e-4b01-8789-c5c8a0c94458",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device) # cpu -> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cd4228a-a589-4251-9455-cd4b3d2899a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbc0b7d2-f72a-4a2e-acee-2243a8d33ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6630,  2.2844,  2.6204,  0.5576, -0.7153],\n",
      "        [ 1.0875,  3.2496,  0.5459,  2.3315, -0.0637],\n",
      "        [ 0.7106,  1.3802,  1.2068,  1.1645,  0.5064]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(z) # Aqui tenemos el z en GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26c409a7-0b7d-4f93-abb9-33cadab1bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6630,  2.2844,  2.6204,  0.5576, -0.7153],\n",
      "        [ 1.0875,  3.2496,  0.5459,  2.3315, -0.0637],\n",
      "        [ 0.7106,  1.3802,  1.2068,  1.1645,  0.5064]])\n"
     ]
    }
   ],
   "source": [
    "print(z.cpu()) # Aqui vemos z desde la CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdec2c-2f9c-4de5-840e-e92273f53228",
   "metadata": {},
   "source": [
    "## Vamos a importar los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea5b4dab-8433-4145-b2fb-8f06017cbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b008071-d603-41d9-85bd-7488e725901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIFAR10',\n",
       " 'CIFAR100',\n",
       " 'Caltech101',\n",
       " 'Caltech256',\n",
       " 'CelebA',\n",
       " 'Cityscapes',\n",
       " 'CocoCaptions',\n",
       " 'CocoDetection',\n",
       " 'DatasetFolder',\n",
       " 'EMNIST',\n",
       " 'FakeData',\n",
       " 'FashionMNIST',\n",
       " 'Flickr30k',\n",
       " 'Flickr8k',\n",
       " 'HMDB51',\n",
       " 'INaturalist',\n",
       " 'ImageFolder',\n",
       " 'ImageNet',\n",
       " 'KMNIST',\n",
       " 'Kinetics',\n",
       " 'Kinetics400',\n",
       " 'Kitti',\n",
       " 'LFWPairs',\n",
       " 'LFWPeople',\n",
       " 'LSUN',\n",
       " 'LSUNClass',\n",
       " 'MNIST',\n",
       " 'Omniglot',\n",
       " 'PhotoTour',\n",
       " 'Places365',\n",
       " 'QMNIST',\n",
       " 'SBDataset',\n",
       " 'SBU',\n",
       " 'SEMEION',\n",
       " 'STL10',\n",
       " 'SVHN',\n",
       " 'UCF101',\n",
       " 'USPS',\n",
       " 'VOCDetection',\n",
       " 'VOCSegmentation',\n",
       " 'VisionDataset',\n",
       " 'WIDERFace',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'caltech',\n",
       " 'celeba',\n",
       " 'cifar',\n",
       " 'cityscapes',\n",
       " 'coco',\n",
       " 'fakedata',\n",
       " 'flickr',\n",
       " 'folder',\n",
       " 'hmdb51',\n",
       " 'imagenet',\n",
       " 'inaturalist',\n",
       " 'kinetics',\n",
       " 'kitti',\n",
       " 'lfw',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'omniglot',\n",
       " 'phototour',\n",
       " 'places365',\n",
       " 'sbd',\n",
       " 'sbu',\n",
       " 'semeion',\n",
       " 'stl10',\n",
       " 'svhn',\n",
       " 'ucf101',\n",
       " 'usps',\n",
       " 'utils',\n",
       " 'video_utils',\n",
       " 'vision',\n",
       " 'voc',\n",
       " 'widerface']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a01761-007a-4cbe-8706-023d7b223d64",
   "metadata": {},
   "source": [
    "### Para instanciar un DS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f93c22a-ad38-45eb-8eaf-f3318421c329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /dl-pytorch/datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97.9%"
     ]
    }
   ],
   "source": [
    "cifar = datasets.CIFAR10('/dl-pytorch/datasets/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94c86734-56a0-4ded-ab09-7227bc1b5776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27034270-68a0-4b02-84cb-c7981c8a52d6",
   "metadata": {},
   "source": [
    "Vamos a pasar el DS a un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05f7a2f4-90de-4ee8-a8d8-869f781d0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.Tensor(cifar.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0952865-bbfc-4a9c-8c9d-27de56a98842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 32, 32, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5a662-c6e1-4de0-8412-11dc6ec06f46",
   "metadata": {},
   "source": [
    "Los datasets de imagenes son de 4 dimensiones.\n",
    "- Primer dimension, ahi se alamacenan la cantidad de imagenes\n",
    "- La dimension x\n",
    "- La dimension y\n",
    "- El canal de imagenes, aqui es RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd151ca-756e-430b-8743-bc6baec9654b",
   "metadata": {},
   "source": [
    "Vamos a ver las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6656432e-32d1-4afa-afd2-9ddc645459d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40a72e7e-fa85-42cc-9e9f-12064f3f6a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a3b61a61d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a3b611c5c0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[2, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f8da2-1457-48a6-942e-20c526d47f08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb918f96-6718-423e-a850-9c863e717b47",
   "metadata": {},
   "source": [
    "## Capas de una red neuronal en Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c81d2775-f2bd-42e2-9afd-6f20db52f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Red Neuronal\n",
    "\n",
    "# Transformacion lineal\n",
    "linear = nn.Linear(in_features=4096, out_features=10) # Aqyu en input solo recibimos tensores de dim 1\n",
    "\n",
    "# Convolucion\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=9, kernel_size=3, stride=1, padding=1)\n",
    "# Esta convolucion modifica los canales pero no el tamanio de la imagen\n",
    "\n",
    "relu = nn.ReLU(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6783237-8763-43ff-af96-d8b8f235ee98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4096)\n",
    "y = linear(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "494a617b-ecfd-436f-8c6a-45b6ce06d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0906, -1.4086, -1.4267,  ...,  0.1467,  1.2375, -1.1456])\n",
      "tensor([ 0.0935,  0.8819,  1.3535,  0.8741,  0.3464,  0.1940,  0.0453, -0.7821,\n",
      "        -0.2806,  0.5633], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61c5fb59-cbe7-4544-a028-aa30a85a2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn(una_imagen, tres_canales, tamanio_x, tamanio_y)\n",
    "x = torch.randn(1, 3, 7, 7)\n",
    "y = conv(x)\n",
    "z = relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0122542-f6a3-40cc-84ee-77bee3186d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 7, 7])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ca95323-b801-425c-ad2d-006d1ee64c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 7, 7])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b8ac14db-6760-40c4-9349-62e4c39e92a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "torch.Size([10, 4096])\n",
      "bias\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, p in linear.named_parameters():\n",
    "    print(name)\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab992f8-94b1-4d32-b45b-93cb697758d5",
   "metadata": {},
   "source": [
    "## Como crear una red neuronal en Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4267500-a4d4-4694-b622-7691966222b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python cHido (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
